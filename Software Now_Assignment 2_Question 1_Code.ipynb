{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data from all CSV files has been successfully merged into '/Users/nishchaltamang/Desktop/consolidated_text.txt'.\n",
      "Top 30 words have been saved to '/Users/nishchaltamang/Desktop/top_30_words.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hit140env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 tokens for /Users/nishchaltamang/Desktop/CSV1.csv have been saved to '/Users/nishchaltamang/Desktop/top_30_tokens_CSV1.csv.csv'.\n",
      "Top 30 tokens for /Users/nishchaltamang/Desktop/CSV2.csv have been saved to '/Users/nishchaltamang/Desktop/top_30_tokens_CSV2.csv.csv'.\n",
      "Top 30 tokens for /Users/nishchaltamang/Desktop/CSV3.csv have been saved to '/Users/nishchaltamang/Desktop/top_30_tokens_CSV3.csv.csv'.\n",
      "Top 30 tokens for /Users/nishchaltamang/Desktop/CSV4.csv have been saved to '/Users/nishchaltamang/Desktop/top_30_tokens_CSV4.csv.csv'.\n",
      "Tokenization for all CSV files is complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Ensure transformers package is installed\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "except ImportError as e:\n",
    "    print(\"Transformers package is not installed. Please run: pip install transformers\")\n",
    "    raise e\n",
    "\n",
    "# Define the path to the CSV files and their respective text columns\n",
    "# Define the path to the CSV files and their respective text columns\n",
    "csv_files = {\n",
    "    \"/Users/nishchaltamang/Desktop/CSV1.csv\": \"SHORT-TEXT\",  \n",
    "    \"/Users/nishchaltamang/Desktop/CSV2.csv\": \"entites\",  \n",
    "    \"/Users/nishchaltamang/Desktop/CSV3.csv\": \"TEXT\",  \n",
    "    \"/Users/nishchaltamang/Desktop/CSV4.csv\": \"TEXT\", \n",
    "}\n",
    "\n",
    "# Set output directory\n",
    "output_dir = \"/Users/nishchaltamang/Desktop/\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Function to consolidate text from each CSV file separately\n",
    "def consolidate_text(csv_file, text_column):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if text_column in df.columns:\n",
    "            return \"\\n\".join(df[text_column].dropna())  # Drop NaN values and join the text\n",
    "        else:\n",
    "            print(f\"Column '{text_column}' not found in {csv_file}. Available columns: {df.columns}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_file}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Consolidate text from each file and store separately\n",
    "consolidated_texts = {}\n",
    "for csv_file, text_column in csv_files.items():\n",
    "    consolidated_texts[csv_file] = consolidate_text(csv_file, text_column)\n",
    "\n",
    "# Merge all the separate texts into a single string\n",
    "final_consolidated_text = \"\\n\".join(consolidated_texts.values())\n",
    "\n",
    "# Task 1: Save the merged consolidated text to a .txt file\n",
    "output_text_file = os.path.join(output_dir, 'consolidated_text.txt')\n",
    "try:\n",
    "    with open(output_text_file, 'w') as f:\n",
    "        f.write(final_consolidated_text)\n",
    "    print(f\"Text data from all CSV files has been successfully merged into '{output_text_file}'.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error writing to file: {e}\")\n",
    "\n",
    "# Task 2: Load the consolidated text from the .txt file\n",
    "try:\n",
    "    with open(output_text_file, 'r') as f:\n",
    "        text = f.read()\n",
    "except OSError as e:\n",
    "    print(f\"Error reading the file: {e}\")\n",
    "    text = \"\"  # Fallback in case of an error\n",
    "\n",
    "# Function to count word occurrences\n",
    "def count_words(text):\n",
    "    words = re.findall(r'\\w+', text.lower())  # Extract words and convert to lowercase\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts\n",
    "\n",
    "# Count the words in the text\n",
    "word_counts = count_words(text)\n",
    "\n",
    "# Get the top 30 most common words\n",
    "top_30_words = word_counts.most_common(30)\n",
    "\n",
    "# Task 3: Save the top 30 words to a CSV file\n",
    "output_top_words_file = os.path.join(output_dir, 'top_30_words.csv')\n",
    "try:\n",
    "    with open(output_top_words_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['word', 'count']\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(fieldnames)\n",
    "        writer.writerows(top_30_words)\n",
    "    print(f\"Top 30 words have been saved to '{output_top_words_file}'.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error writing the file: {e}\")\n",
    "\n",
    "\n",
    "# Function to tokenize text in chunks (This function splits the text into smaller chunks, each with a length of up to 512 tokens, which is the limit for BERT.)\n",
    "\n",
    "def tokenize_in_chunks(text, chunk_size=512):\n",
    "    tokens = []\n",
    "    # Split the text into chunks of the maximum length allowed by the model\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        tokens.extend(tokenizer.tokenize(chunk))  # Tokenize each chunk and extend the token list\n",
    "    return tokens\n",
    "\n",
    "# Part 4: Tokenization and Top 30 Tokens for each CSV file\n",
    "# Load the tokenizer for BERT\n",
    "# Function to tokenize text in chunks (This function splits the text into smaller chunks, each with a length of up to 512 tokens, which is the limit for BERT.)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to count token occurrences from individual texts\n",
    "def count_tokens(text, chunk_size=512):\n",
    "    tokens = tokenize_in_chunks(text, chunk_size)\n",
    "    token_counts = Counter(tokens)\n",
    "    return token_counts\n",
    "\n",
    "# Iterate over each CSV file and tokenize its respective text column\n",
    "for csv_file, text_column in csv_files.items():\n",
    "    csv_text = consolidated_texts[csv_file]  # Get text from the specific CSV file's column\n",
    "    \n",
    "    if csv_text:\n",
    "        # Tokenize the text\n",
    "        token_counts = count_tokens(csv_text)\n",
    "        \n",
    "        # Get the top 30 most common tokens for the current CSV file\n",
    "        top_30_tokens = token_counts.most_common(30)\n",
    "        \n",
    "        # Save the top 30 tokens to a separate CSV file for each input CSV\n",
    "        token_output_file = os.path.join(output_dir, f'top_30_tokens_{os.path.basename(csv_file)}.csv')\n",
    "        try:\n",
    "            with open(token_output_file, 'w', newline='') as csvfile:\n",
    "                fieldnames = ['token', 'count']\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(fieldnames)\n",
    "                writer.writerows(top_30_tokens)\n",
    "            print(f\"Top 30 tokens for {csv_file} have been saved to '{token_output_file}'.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error writing the file for {csv_file}: {e}\")\n",
    "\n",
    "print(\"Tokenization for all CSV files is complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hit140env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
